[2024-03-20T05:13:38.705+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.from_gcs_to_bq manual__2024-03-20T05:04:30.882224+00:00 [queued]>
[2024-03-20T05:13:38.709+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.from_gcs_to_bq manual__2024-03-20T05:04:30.882224+00:00 [queued]>
[2024-03-20T05:13:38.709+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-03-20T05:13:38.714+0000] {taskinstance.py:2217} INFO - Executing <Task(BashOperator): from_gcs_to_bq> on 2024-03-20 05:04:30.882224+00:00
[2024-03-20T05:13:38.716+0000] {standard_task_runner.py:60} INFO - Started process 1576 to run task
[2024-03-20T05:13:38.718+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'IDM_TO_BQ_PQ_RAW', 'from_gcs_to_bq', 'manual__2024-03-20T05:04:30.882224+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/IDM_TO_BQ_PQ_RAW.py', '--cfg-path', '/tmp/tmp5juzejwe']
[2024-03-20T05:13:38.719+0000] {standard_task_runner.py:88} INFO - Job 27: Subtask from_gcs_to_bq
[2024-03-20T05:13:38.727+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-03-20T05:13:38.740+0000] {task_command.py:423} INFO - Running <TaskInstance: IDM_TO_BQ_PQ_RAW.from_gcs_to_bq manual__2024-03-20T05:04:30.882224+00:00 [running]> on host bf143d508a6e
[2024-03-20T05:13:38.764+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='admin' AIRFLOW_CTX_DAG_ID='IDM_TO_BQ_PQ_RAW' AIRFLOW_CTX_TASK_ID='from_gcs_to_bq' AIRFLOW_CTX_EXECUTION_DATE='2024-03-20T05:04:30.882224+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-03-20T05:04:30.882224+00:00'
[2024-03-20T05:13:38.764+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-03-20T05:13:38.765+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', "#!/bin/bash\n\nSERVICE_ACCOUNT_FILE_NAME=$GOOGLE_APPLICATION_CREDENTIALS\nPYTHON_DIR=/opt/***/src/idm_to_bq_pq_raw/pyspark/\nPYTHON_FILE=from_gcs_to_bq.py\nDATAPROC_CLUSTER=dez-cluster-emil\nDATAPROC_REGION=us-central1\nGCP_PROJECT=dez-workspace-emil\n\necho\necho 'SETTING UP SERVICE ACCOUNT'\ngcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_FILE_NAME\ngcloud config set project $GCP_PROJECT\n\necho\necho 'STARTING PQ TO BQ TABLE'\n\ngcloud dataproc jobs submit pyspark $PYTHON_DIR$PYTHON_FILE --cluster=$DATAPROC_CLUSTER --region=$DATAPROC_REGION\n\necho\necho 'DONE!'"]
[2024-03-20T05:13:38.768+0000] {subprocess.py:86} INFO - Output:
[2024-03-20T05:13:38.769+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:13:38.769+0000] {subprocess.py:93} INFO - SETTING UP SERVICE ACCOUNT
[2024-03-20T05:13:39.415+0000] {subprocess.py:93} INFO - Activated service account credentials for: [dez-project-emil@dez-workspace-emil.iam.gserviceaccount.com]
[2024-03-20T05:13:40.036+0000] {subprocess.py:93} INFO - WARNING: You do not appear to have access to project [dez-workspace-emil] or it does not exist.
[2024-03-20T05:13:40.038+0000] {subprocess.py:93} INFO - Updated property [core/project].
[2024-03-20T05:13:40.112+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:13:40.113+0000] {subprocess.py:93} INFO - STARTING PQ TO BQ TABLE
[2024-03-20T05:13:41.822+0000] {subprocess.py:93} INFO - Job [7d80290a7e144db4a35c6ccdb1d0db50] submitted.
[2024-03-20T05:13:41.824+0000] {subprocess.py:93} INFO - Waiting for job output...
[2024-03-20T05:13:52.270+0000] {subprocess.py:93} INFO - 24/03/20 05:13:49 INFO SparkEnv: Registering MapOutputTracker
[2024-03-20T05:13:52.272+0000] {subprocess.py:93} INFO - 24/03/20 05:13:49 INFO SparkEnv: Registering BlockManagerMaster
[2024-03-20T05:13:52.273+0000] {subprocess.py:93} INFO - 24/03/20 05:13:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-03-20T05:13:52.273+0000] {subprocess.py:93} INFO - 24/03/20 05:13:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-03-20T05:13:52.274+0000] {subprocess.py:93} INFO - 24/03/20 05:13:50 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:8032
[2024-03-20T05:13:52.275+0000] {subprocess.py:93} INFO - 24/03/20 05:13:50 INFO AHSProxy: Connecting to Application History server at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:10200
[2024-03-20T05:13:54.717+0000] {subprocess.py:93} INFO - 24/03/20 05:13:52 INFO Configuration: resource-types.xml not found
[2024-03-20T05:13:54.719+0000] {subprocess.py:93} INFO - 24/03/20 05:13:52 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[2024-03-20T05:13:57.476+0000] {subprocess.py:93} INFO - 24/03/20 05:13:53 INFO YarnClientImpl: Submitted application application_1710911145524_0002
[2024-03-20T05:13:57.479+0000] {subprocess.py:93} INFO - 24/03/20 05:13:54 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:8030
[2024-03-20T05:14:00.039+0000] {subprocess.py:93} INFO - 24/03/20 05:13:57 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=502; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history
[2024-03-20T05:14:00.042+0000] {subprocess.py:93} INFO - 24/03/20 05:13:57 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2024-03-20T05:14:00.043+0000] {subprocess.py:93} INFO - 24/03/20 05:13:57 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=265; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history
[2024-03-20T05:14:00.044+0000] {subprocess.py:93} INFO - 24/03/20 05:13:57 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=162; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history/application_1710911145524_0002.inprogress
[2024-03-20T05:14:03.566+0000] {subprocess.py:93} INFO - READING PARQUET FILE
[2024-03-20T05:14:03.569+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-03-20T05:14:03.571+0000] {subprocess.py:93} INFO -   File "/tmp/7d80290a7e144db4a35c6ccdb1d0db50/from_gcs_to_bq.py", line 20, in <module>
[2024-03-20T05:14:03.572+0000] {subprocess.py:93} INFO -     idm_df = spark.read.parquet('gs://landing_bucket_dez/pq/idm/*')
[2024-03-20T05:14:03.574+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 364, in parquet
[2024-03-20T05:14:03.578+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2024-03-20T05:14:03.581+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2024-03-20T05:14:03.582+0000] {subprocess.py:93} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: gs://landing_bucket_dez/pq/idm/*
[2024-03-20T05:14:12.014+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [7d80290a7e144db4a35c6ccdb1d0db50] failed with error:
[2024-03-20T05:14:12.017+0000] {subprocess.py:93} INFO - Job failed with message [pyspark.sql.utils.AnalysisException: Path does not exist: gs://landing_bucket_dez/pq/idm/*]. Additional details can be found at:
[2024-03-20T05:14:12.018+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/7d80290a7e144db4a35c6ccdb1d0db50?project=dez-workspace-emil&region=us-central1
[2024-03-20T05:14:12.019+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait '7d80290a7e144db4a35c6ccdb1d0db50' --region 'us-central1' --project 'dez-workspace-emil'
[2024-03-20T05:14:12.019+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/0d535e65-dd60-4b02-909b-d22e134b1b15/jobs/7d80290a7e144db4a35c6ccdb1d0db50/
[2024-03-20T05:14:12.020+0000] {subprocess.py:93} INFO - gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/0d535e65-dd60-4b02-909b-d22e134b1b15/jobs/7d80290a7e144db4a35c6ccdb1d0db50/driveroutput.*
[2024-03-20T05:14:12.121+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:14:12.122+0000] {subprocess.py:93} INFO - DONE!
[2024-03-20T05:14:12.122+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2024-03-20T05:14:12.142+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=IDM_TO_BQ_PQ_RAW, task_id=from_gcs_to_bq, execution_date=20240320T050430, start_date=20240320T051338, end_date=20240320T051412
[2024-03-20T05:14:12.193+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-03-20T05:14:12.208+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
