[2024-03-20T05:14:58.075+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:04:30.882224+00:00 [queued]>
[2024-03-20T05:14:58.079+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:04:30.882224+00:00 [queued]>
[2024-03-20T05:14:58.079+0000] {taskinstance.py:2193} INFO - Starting attempt 2 of 2
[2024-03-20T05:14:58.084+0000] {taskinstance.py:2217} INFO - Executing <Task(BashOperator): csv_to_pq> on 2024-03-20 05:04:30.882224+00:00
[2024-03-20T05:14:58.087+0000] {standard_task_runner.py:60} INFO - Started process 1699 to run task
[2024-03-20T05:14:58.089+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'IDM_TO_BQ_PQ_RAW', 'csv_to_pq', 'manual__2024-03-20T05:04:30.882224+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/IDM_TO_BQ_PQ_RAW.py', '--cfg-path', '/tmp/tmp4dsy2jqn']
[2024-03-20T05:14:58.090+0000] {standard_task_runner.py:88} INFO - Job 29: Subtask csv_to_pq
[2024-03-20T05:14:58.100+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-03-20T05:14:58.113+0000] {task_command.py:423} INFO - Running <TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:04:30.882224+00:00 [running]> on host bf143d508a6e
[2024-03-20T05:14:58.138+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='admin' AIRFLOW_CTX_DAG_ID='IDM_TO_BQ_PQ_RAW' AIRFLOW_CTX_TASK_ID='csv_to_pq' AIRFLOW_CTX_EXECUTION_DATE='2024-03-20T05:04:30.882224+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-03-20T05:04:30.882224+00:00'
[2024-03-20T05:14:58.139+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-03-20T05:14:58.140+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', "#!/bin/bash\n\nSERVICE_ACCOUNT_FILE_NAME=$GOOGLE_APPLICATION_CREDENTIALS\nPYTHON_DIR=/opt/***/src/idm_to_bq_pq_raw/pyspark/\nPYTHON_FILE=csv_to_pq.py\nDATAPROC_CLUSTER=dez-cluster-emil\nDATAPROC_REGION=us-central1\nGCP_PROJECT=dez-workspace-emil\n\necho\necho 'SETTING UP SERVICE ACCOUNT'\ngcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_FILE_NAME\ngcloud config set project $GCP_PROJECT\n\necho\necho 'STARTING FROM CSV TO PARQUET'\n\ngcloud dataproc jobs submit pyspark $PYTHON_DIR$PYTHON_FILE --cluster=$DATAPROC_CLUSTER --region=$DATAPROC_REGION\n\necho\necho 'DONE!'"]
[2024-03-20T05:14:58.143+0000] {subprocess.py:86} INFO - Output:
[2024-03-20T05:14:58.144+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:14:58.145+0000] {subprocess.py:93} INFO - SETTING UP SERVICE ACCOUNT
[2024-03-20T05:14:58.803+0000] {subprocess.py:93} INFO - Activated service account credentials for: [dez-project-emil@dez-workspace-emil.iam.gserviceaccount.com]
[2024-03-20T05:14:59.434+0000] {subprocess.py:93} INFO - WARNING: You do not appear to have access to project [dez-workspace-emil] or it does not exist.
[2024-03-20T05:14:59.437+0000] {subprocess.py:93} INFO - Updated property [core/project].
[2024-03-20T05:14:59.528+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:14:59.529+0000] {subprocess.py:93} INFO - STARTING FROM CSV TO PARQUET
[2024-03-20T05:15:01.171+0000] {subprocess.py:93} INFO - Job [9bea27a507a24e6eb67f917e4b5bb397] submitted.
[2024-03-20T05:15:01.172+0000] {subprocess.py:93} INFO - Waiting for job output...
[2024-03-20T05:15:09.193+0000] {subprocess.py:93} INFO - 24/03/20 05:15:06 INFO SparkEnv: Registering MapOutputTracker
[2024-03-20T05:15:09.194+0000] {subprocess.py:93} INFO - 24/03/20 05:15:06 INFO SparkEnv: Registering BlockManagerMaster
[2024-03-20T05:15:09.194+0000] {subprocess.py:93} INFO - 24/03/20 05:15:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-03-20T05:15:09.194+0000] {subprocess.py:93} INFO - 24/03/20 05:15:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-03-20T05:15:09.195+0000] {subprocess.py:93} INFO - 24/03/20 05:15:07 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:8032
[2024-03-20T05:15:09.195+0000] {subprocess.py:93} INFO - 24/03/20 05:15:07 INFO AHSProxy: Connecting to Application History server at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:10200
[2024-03-20T05:15:11.718+0000] {subprocess.py:93} INFO - 24/03/20 05:15:08 INFO Configuration: resource-types.xml not found
[2024-03-20T05:15:11.720+0000] {subprocess.py:93} INFO - 24/03/20 05:15:08 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[2024-03-20T05:15:11.721+0000] {subprocess.py:93} INFO - 24/03/20 05:15:09 INFO YarnClientImpl: Submitted application application_1710911145524_0003
[2024-03-20T05:15:11.722+0000] {subprocess.py:93} INFO - 24/03/20 05:15:10 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-emil-m.us-central1-c.c.dez-workspace-emil.internal./10.128.0.3:8030
[2024-03-20T05:15:15.864+0000] {subprocess.py:93} INFO - 24/03/20 05:15:12 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=367; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history
[2024-03-20T05:15:15.866+0000] {subprocess.py:93} INFO - 24/03/20 05:15:12 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2024-03-20T05:15:15.867+0000] {subprocess.py:93} INFO - 24/03/20 05:15:12 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=228; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history
[2024-03-20T05:15:15.867+0000] {subprocess.py:93} INFO - 24/03/20 05:15:13 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=180; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/0d535e65-dd60-4b02-909b-d22e134b1b15/spark-job-history/application_1710911145524_0003.inprogress
[2024-03-20T05:15:15.868+0000] {subprocess.py:93} INFO - READING CSV FILE
[2024-03-20T05:15:22.051+0000] {subprocess.py:93} INFO - TRANSFORMING DATA
[2024-03-20T05:15:24.439+0000] {subprocess.py:93} INFO - WRITING DATA
[2024-03-20T05:15:27.078+0000] {subprocess.py:93} INFO - 24/03/20 05:15:23 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=251; previousMaxLatencyMs=0; operationCount=1; context=gs://landing_bucket_dez/pq/idm
[2024-03-20T05:16:34.663+0000] {subprocess.py:93} INFO - 24/03/20 05:16:32 ERROR FileFormatWriter: Aborting job 5e4238bc-16e2-4b85-972a-014484bacf97.
[2024-03-20T05:16:34.667+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
[2024-03-20T05:16:34.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1195) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1193) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.668+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79) ~[scala-library-2.12.18.jar:?]
[2024-03-20T05:16:34.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1193) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2940) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2834) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2834) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2158) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2158) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2112) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.673+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:710) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.673+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.674+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
[2024-03-20T05:16:34.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.675+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.675+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
[2024-03-20T05:16:34.676+0000] {subprocess.py:93} INFO - 	at scala.util.Try$.apply(Try.scala:213) ~[scala-library-2.12.18.jar:?]
[2024-03-20T05:16:34.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.677+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
[2024-03-20T05:16:34.677+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
[2024-03-20T05:16:34.677+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
[2024-03-20T05:16:34.678+0000] {subprocess.py:93} INFO - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
[2024-03-20T05:16:34.678+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:829) ~[?:?]
[2024-03-20T05:16:34.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:257) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.684+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.684+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.684+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.685+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.685+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.686+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.686+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.686+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.687+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.687+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.687+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.690+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.690+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.690+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T05:16:34.691+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
[2024-03-20T05:16:34.691+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
[2024-03-20T05:16:34.692+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
[2024-03-20T05:16:34.692+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
[2024-03-20T05:16:34.692+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.692+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.693+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.693+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.693+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.694+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.694+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T05:16:34.694+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:829) ~[?:?]
[2024-03-20T05:16:36.493+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-03-20T05:16:36.495+0000] {subprocess.py:93} INFO -   File "/tmp/9bea27a507a24e6eb67f917e4b5bb397/csv_to_pq.py", line 108, in <module>
[2024-03-20T05:16:36.496+0000] {subprocess.py:93} INFO -     .parquet('gs://landing_bucket_dez/pq/idm/')
[2024-03-20T05:16:36.496+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1140, in parquet
[2024-03-20T05:16:36.497+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2024-03-20T05:16:36.497+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2024-03-20T05:16:36.498+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2024-03-20T05:16:36.498+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o112.parquet.
[2024-03-20T05:16:36.499+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted.
[2024-03-20T05:16:36.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:650)
[2024-03-20T05:16:36.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:309)
[2024-03-20T05:16:36.500+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2024-03-20T05:16:36.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2024-03-20T05:16:36.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2024-03-20T05:16:36.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2024-03-20T05:16:36.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2024-03-20T05:16:36.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2024-03-20T05:16:36.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2024-03-20T05:16:36.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2024-03-20T05:16:36.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2024-03-20T05:16:36.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2024-03-20T05:16:36.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2024-03-20T05:16:36.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2024-03-20T05:16:36.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2024-03-20T05:16:36.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2024-03-20T05:16:36.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2024-03-20T05:16:36.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T05:16:36.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2024-03-20T05:16:36.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2024-03-20T05:16:36.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T05:16:36.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T05:16:36.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2024-03-20T05:16:36.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2024-03-20T05:16:36.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2024-03-20T05:16:36.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2024-03-20T05:16:36.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2024-03-20T05:16:36.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2024-03-20T05:16:36.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2024-03-20T05:16:36.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2024-03-20T05:16:36.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2024-03-20T05:16:36.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)
[2024-03-20T05:16:36.514+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-03-20T05:16:36.515+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2024-03-20T05:16:36.515+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-03-20T05:16:36.515+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2024-03-20T05:16:36.516+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2024-03-20T05:16:36.516+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2024-03-20T05:16:36.516+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2024-03-20T05:16:36.517+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2024-03-20T05:16:36.517+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2024-03-20T05:16:36.517+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2024-03-20T05:16:36.518+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2024-03-20T05:16:36.518+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2024-03-20T05:16:36.518+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
[2024-03-20T05:16:36.518+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1195)
[2024-03-20T05:16:36.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1193)
[2024-03-20T05:16:36.519+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
[2024-03-20T05:16:36.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1193)
[2024-03-20T05:16:36.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2940)
[2024-03-20T05:16:36.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
[2024-03-20T05:16:36.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2834)
[2024-03-20T05:16:36.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)
[2024-03-20T05:16:36.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2834)
[2024-03-20T05:16:36.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2158)
[2024-03-20T05:16:36.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1485)
[2024-03-20T05:16:36.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2158)
[2024-03-20T05:16:36.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2112)
[2024-03-20T05:16:36.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:710)
[2024-03-20T05:16:36.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
[2024-03-20T05:16:36.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
[2024-03-20T05:16:36.523+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-03-20T05:16:36.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
[2024-03-20T05:16:36.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
[2024-03-20T05:16:36.524+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-03-20T05:16:36.524+0000] {subprocess.py:93} INFO - 	at scala.util.Try$.apply(Try.scala:213)
[2024-03-20T05:16:36.525+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
[2024-03-20T05:16:36.525+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
[2024-03-20T05:16:36.525+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-03-20T05:16:36.525+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2024-03-20T05:16:36.526+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-03-20T05:16:36.526+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-03-20T05:16:36.526+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2024-03-20T05:16:36.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)
[2024-03-20T05:16:36.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2024-03-20T05:16:36.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:257)
[2024-03-20T05:16:36.527+0000] {subprocess.py:93} INFO - 	... 42 more
[2024-03-20T05:16:36.528+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:16:38.233+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [9bea27a507a24e6eb67f917e4b5bb397] failed with error:
[2024-03-20T05:16:38.234+0000] {subprocess.py:93} INFO - Driver received SIGTERM/SIGKILL signal and exited with 143 code, which potentially signifies a memory pressure.
[2024-03-20T05:16:38.234+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2024-03-20T05:16:38.234+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/9bea27a507a24e6eb67f917e4b5bb397?project=dez-workspace-emil&region=us-central1
[2024-03-20T05:16:38.235+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait '9bea27a507a24e6eb67f917e4b5bb397' --region 'us-central1' --project 'dez-workspace-emil'
[2024-03-20T05:16:38.235+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/0d535e65-dd60-4b02-909b-d22e134b1b15/jobs/9bea27a507a24e6eb67f917e4b5bb397/
[2024-03-20T05:16:38.236+0000] {subprocess.py:93} INFO - gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/0d535e65-dd60-4b02-909b-d22e134b1b15/jobs/9bea27a507a24e6eb67f917e4b5bb397/driveroutput.*
[2024-03-20T05:16:38.317+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:16:38.318+0000] {subprocess.py:93} INFO - DONE!
[2024-03-20T05:16:38.318+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2024-03-20T05:16:38.350+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=IDM_TO_BQ_PQ_RAW, task_id=csv_to_pq, execution_date=20240320T050430, start_date=20240320T051458, end_date=20240320T051638
[2024-03-20T05:16:38.364+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-03-20T05:16:38.379+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
