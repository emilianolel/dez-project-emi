[2024-03-20T05:57:20.558+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:41:25.524027+00:00 [queued]>
[2024-03-20T05:57:20.562+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:41:25.524027+00:00 [queued]>
[2024-03-20T05:57:20.562+0000] {taskinstance.py:2193} INFO - Starting attempt 2 of 2
[2024-03-20T05:57:20.567+0000] {taskinstance.py:2217} INFO - Executing <Task(BashOperator): csv_to_pq> on 2024-03-20 05:41:25.524027+00:00
[2024-03-20T05:57:20.569+0000] {standard_task_runner.py:60} INFO - Started process 5589 to run task
[2024-03-20T05:57:20.571+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'IDM_TO_BQ_PQ_RAW', 'csv_to_pq', 'manual__2024-03-20T05:41:25.524027+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/IDM_TO_BQ_PQ_RAW.py', '--cfg-path', '/tmp/tmpuhfn6_6f']
[2024-03-20T05:57:20.572+0000] {standard_task_runner.py:88} INFO - Job 38: Subtask csv_to_pq
[2024-03-20T05:57:20.581+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-03-20T05:57:20.593+0000] {task_command.py:423} INFO - Running <TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq manual__2024-03-20T05:41:25.524027+00:00 [running]> on host bf143d508a6e
[2024-03-20T05:57:20.618+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='admin' AIRFLOW_CTX_DAG_ID='IDM_TO_BQ_PQ_RAW' AIRFLOW_CTX_TASK_ID='csv_to_pq' AIRFLOW_CTX_EXECUTION_DATE='2024-03-20T05:41:25.524027+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-03-20T05:41:25.524027+00:00'
[2024-03-20T05:57:20.619+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-03-20T05:57:20.619+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', "#!/bin/bash\n\nSERVICE_ACCOUNT_FILE_NAME=$GOOGLE_APPLICATION_CREDENTIALS\nPYTHON_DIR=/opt/***/src/idm_to_bq_pq_raw/pyspark/\nPYTHON_FILE=csv_to_pq.py\nDATAPROC_CLUSTER=dez-cluster\nDATAPROC_REGION=us-central1\nGCP_PROJECT=dez-workspace-emil\n\necho\necho 'SETTING UP SERVICE ACCOUNT'\ngcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_FILE_NAME\ngcloud config set project $GCP_PROJECT\n\necho\necho 'STARTING FROM CSV TO PARQUET'\n\ngcloud dataproc jobs submit pyspark $PYTHON_DIR$PYTHON_FILE --cluster=$DATAPROC_CLUSTER --region=$DATAPROC_REGION\n\necho\necho 'DONE!'"]
[2024-03-20T05:57:20.623+0000] {subprocess.py:86} INFO - Output:
[2024-03-20T05:57:20.624+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:57:20.624+0000] {subprocess.py:93} INFO - SETTING UP SERVICE ACCOUNT
[2024-03-20T05:57:21.311+0000] {subprocess.py:93} INFO - Activated service account credentials for: [dez-project-emil@dez-workspace-emil.iam.gserviceaccount.com]
[2024-03-20T05:57:21.998+0000] {subprocess.py:93} INFO - WARNING: You do not appear to have access to project [dez-workspace-emil] or it does not exist.
[2024-03-20T05:57:21.999+0000] {subprocess.py:93} INFO - Updated property [core/project].
[2024-03-20T05:57:22.058+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:57:22.058+0000] {subprocess.py:93} INFO - STARTING FROM CSV TO PARQUET
[2024-03-20T05:57:23.903+0000] {subprocess.py:93} INFO - Job [08e180f542834810a49f2716136b3109] submitted.
[2024-03-20T05:57:23.904+0000] {subprocess.py:93} INFO - Waiting for job output...
[2024-03-20T05:57:34.932+0000] {subprocess.py:93} INFO - 24/03/20 05:57:32 INFO SparkEnv: Registering MapOutputTracker
[2024-03-20T05:57:34.939+0000] {subprocess.py:93} INFO - 24/03/20 05:57:32 INFO SparkEnv: Registering BlockManagerMaster
[2024-03-20T05:57:34.941+0000] {subprocess.py:93} INFO - 24/03/20 05:57:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-03-20T05:57:34.945+0000] {subprocess.py:93} INFO - 24/03/20 05:57:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-03-20T05:57:37.633+0000] {subprocess.py:93} INFO - 24/03/20 05:57:34 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:8032
[2024-03-20T05:57:37.635+0000] {subprocess.py:93} INFO - 24/03/20 05:57:34 INFO AHSProxy: Connecting to Application History server at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:10200
[2024-03-20T05:57:39.868+0000] {subprocess.py:93} INFO - 24/03/20 05:57:36 INFO Configuration: resource-types.xml not found
[2024-03-20T05:57:39.870+0000] {subprocess.py:93} INFO - 24/03/20 05:57:36 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[2024-03-20T05:57:39.871+0000] {subprocess.py:93} INFO - 24/03/20 05:57:38 INFO YarnClientImpl: Submitted application application_1710913474836_0003
[2024-03-20T05:57:39.871+0000] {subprocess.py:93} INFO - 24/03/20 05:57:39 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:8030
[2024-03-20T05:57:43.836+0000] {subprocess.py:93} INFO - 24/03/20 05:57:41 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=428; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/830261e5-2d3d-4491-a80c-fd201b1ad541/spark-job-history
[2024-03-20T05:57:43.837+0000] {subprocess.py:93} INFO - 24/03/20 05:57:41 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2024-03-20T05:57:43.837+0000] {subprocess.py:93} INFO - 24/03/20 05:57:41 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=187; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/830261e5-2d3d-4491-a80c-fd201b1ad541/spark-job-history
[2024-03-20T05:57:45.788+0000] {subprocess.py:93} INFO - READING CSV FILE
[2024-03-20T05:57:51.206+0000] {subprocess.py:93} INFO - TRANSFORMING DATA
[2024-03-20T05:57:52.922+0000] {subprocess.py:93} INFO - WRITING DATA
[2024-03-20T05:57:55.056+0000] {subprocess.py:93} INFO - 24/03/20 05:57:52 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=641; previousMaxLatencyMs=0; operationCount=1; context=gs://landing_bucket_dez/pq/idm
[2024-03-20T05:59:18.969+0000] {subprocess.py:93} INFO - 24/03/20 05:59:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://landing_bucket_dez/pq/idm/' directory.
[2024-03-20T05:59:18.974+0000] {subprocess.py:93} INFO - 24/03/20 05:59:16 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=212; previousMaxLatencyMs=0; operationCount=1; context=gs://landing_bucket_dez/pq/idm/_SUCCESS
[2024-03-20T05:59:18.975+0000] {subprocess.py:93} INFO - DONE!
[2024-03-20T05:59:20.630+0000] {subprocess.py:93} INFO - Job [08e180f542834810a49f2716136b3109] finished successfully.
[2024-03-20T05:59:20.657+0000] {subprocess.py:93} INFO - done: true
[2024-03-20T05:59:20.658+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/830261e5-2d3d-4491-a80c-fd201b1ad541/jobs/08e180f542834810a49f2716136b3109/
[2024-03-20T05:59:20.658+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/830261e5-2d3d-4491-a80c-fd201b1ad541/jobs/08e180f542834810a49f2716136b3109/driveroutput
[2024-03-20T05:59:20.659+0000] {subprocess.py:93} INFO - jobUuid: fedb294b-f708-39d9-ad82-6a9c6852c461
[2024-03-20T05:59:20.659+0000] {subprocess.py:93} INFO - placement:
[2024-03-20T05:59:20.660+0000] {subprocess.py:93} INFO -   clusterName: dez-cluster
[2024-03-20T05:59:20.660+0000] {subprocess.py:93} INFO -   clusterUuid: 830261e5-2d3d-4491-a80c-fd201b1ad541
[2024-03-20T05:59:20.660+0000] {subprocess.py:93} INFO - pysparkJob:
[2024-03-20T05:59:20.661+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/830261e5-2d3d-4491-a80c-fd201b1ad541/jobs/08e180f542834810a49f2716136b3109/staging/csv_to_pq.py
[2024-03-20T05:59:20.661+0000] {subprocess.py:93} INFO - reference:
[2024-03-20T05:59:20.662+0000] {subprocess.py:93} INFO -   jobId: 08e180f542834810a49f2716136b3109
[2024-03-20T05:59:20.662+0000] {subprocess.py:93} INFO -   projectId: dez-workspace-emil
[2024-03-20T05:59:20.662+0000] {subprocess.py:93} INFO - status:
[2024-03-20T05:59:20.663+0000] {subprocess.py:93} INFO -   state: DONE
[2024-03-20T05:59:20.663+0000] {subprocess.py:93} INFO -   stateStartTime: '2024-03-20T05:59:18.060708Z'
[2024-03-20T05:59:20.663+0000] {subprocess.py:93} INFO - statusHistory:
[2024-03-20T05:59:20.664+0000] {subprocess.py:93} INFO - - state: PENDING
[2024-03-20T05:59:20.664+0000] {subprocess.py:93} INFO -   stateStartTime: '2024-03-20T05:57:23.847516Z'
[2024-03-20T05:59:20.665+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2024-03-20T05:59:20.665+0000] {subprocess.py:93} INFO -   stateStartTime: '2024-03-20T05:57:23.875070Z'
[2024-03-20T05:59:20.666+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2024-03-20T05:59:20.666+0000] {subprocess.py:93} INFO -   state: RUNNING
[2024-03-20T05:59:20.666+0000] {subprocess.py:93} INFO -   stateStartTime: '2024-03-20T05:57:24.096039Z'
[2024-03-20T05:59:20.667+0000] {subprocess.py:93} INFO - yarnApplications:
[2024-03-20T05:59:20.667+0000] {subprocess.py:93} INFO - - name: test
[2024-03-20T05:59:20.667+0000] {subprocess.py:93} INFO -   progress: 1.0
[2024-03-20T05:59:20.668+0000] {subprocess.py:93} INFO -   state: FINISHED
[2024-03-20T05:59:20.668+0000] {subprocess.py:93} INFO -   trackingUrl: http://dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal.:8088/proxy/application_1710913474836_0003/
[2024-03-20T05:59:20.739+0000] {subprocess.py:93} INFO - 
[2024-03-20T05:59:20.739+0000] {subprocess.py:93} INFO - DONE!
[2024-03-20T05:59:20.740+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2024-03-20T05:59:20.760+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=IDM_TO_BQ_PQ_RAW, task_id=csv_to_pq, execution_date=20240320T054125, start_date=20240320T055720, end_date=20240320T055920
[2024-03-20T05:59:20.773+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-03-20T05:59:20.788+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
