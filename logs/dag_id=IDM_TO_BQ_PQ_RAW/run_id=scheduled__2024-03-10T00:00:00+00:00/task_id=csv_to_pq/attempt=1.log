[2024-03-20T06:28:04.326+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq scheduled__2024-03-10T00:00:00+00:00 [queued]>
[2024-03-20T06:28:04.330+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq scheduled__2024-03-10T00:00:00+00:00 [queued]>
[2024-03-20T06:28:04.330+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 1
[2024-03-20T06:28:04.335+0000] {taskinstance.py:2217} INFO - Executing <Task(BashOperator): csv_to_pq> on 2024-03-10 00:00:00+00:00
[2024-03-20T06:28:04.337+0000] {standard_task_runner.py:60} INFO - Started process 1388 to run task
[2024-03-20T06:28:04.340+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'IDM_TO_BQ_PQ_RAW', 'csv_to_pq', 'scheduled__2024-03-10T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/IDM_TO_BQ_PQ_RAW.py', '--cfg-path', '/tmp/tmptg77uth5']
[2024-03-20T06:28:04.341+0000] {standard_task_runner.py:88} INFO - Job 4: Subtask csv_to_pq
[2024-03-20T06:28:04.354+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:194 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-03-20T06:28:04.371+0000] {task_command.py:423} INFO - Running <TaskInstance: IDM_TO_BQ_PQ_RAW.csv_to_pq scheduled__2024-03-10T00:00:00+00:00 [running]> on host bc8b6df1052f
[2024-03-20T06:28:04.397+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='admin' AIRFLOW_CTX_DAG_ID='IDM_TO_BQ_PQ_RAW' AIRFLOW_CTX_TASK_ID='csv_to_pq' AIRFLOW_CTX_EXECUTION_DATE='2024-03-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-03-10T00:00:00+00:00'
[2024-03-20T06:28:04.398+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-03-20T06:28:04.398+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', "#!/bin/bash\n\nSERVICE_ACCOUNT_FILE_NAME=$GOOGLE_APPLICATION_CREDENTIALS\nPYTHON_DIR=/opt/***/src/idm_to_bq_pq_raw/pyspark/\nPYTHON_FILE=csv_to_pq.py\nDATAPROC_CLUSTER=dez-cluster\nDATAPROC_REGION=us-central1\nGCP_PROJECT=dez-workspace-emil\n\necho\necho 'SETTING UP SERVICE ACCOUNT'\ngcloud auth activate-service-account --key-file=$SERVICE_ACCOUNT_FILE_NAME\ngcloud config set project $GCP_PROJECT\n\necho\necho 'STARTING FROM CSV TO PARQUET'\n\ngcloud dataproc jobs submit pyspark $PYTHON_DIR$PYTHON_FILE --cluster=$DATAPROC_CLUSTER --region=$DATAPROC_REGION\n\necho\necho 'DONE!'"]
[2024-03-20T06:28:04.402+0000] {subprocess.py:86} INFO - Output:
[2024-03-20T06:28:04.403+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:04.403+0000] {subprocess.py:93} INFO - SETTING UP SERVICE ACCOUNT
[2024-03-20T06:28:05.077+0000] {subprocess.py:93} INFO - Activated service account credentials for: [dez-project-emil@dez-workspace-emil.iam.gserviceaccount.com]
[2024-03-20T06:28:05.904+0000] {subprocess.py:93} INFO - WARNING: You do not appear to have access to project [dez-workspace-emil] or it does not exist.
[2024-03-20T06:28:05.906+0000] {subprocess.py:93} INFO - Updated property [core/project].
[2024-03-20T06:28:05.993+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:05.994+0000] {subprocess.py:93} INFO - STARTING FROM CSV TO PARQUET
[2024-03-20T06:28:07.827+0000] {subprocess.py:93} INFO - Job [7c37081032174b04ac27a2123f3430be] submitted.
[2024-03-20T06:28:07.828+0000] {subprocess.py:93} INFO - Waiting for job output...
[2024-03-20T06:28:23.122+0000] {subprocess.py:93} INFO - 24/03/20 06:28:19 INFO SparkEnv: Registering MapOutputTracker
[2024-03-20T06:28:23.125+0000] {subprocess.py:93} INFO - 24/03/20 06:28:19 INFO SparkEnv: Registering BlockManagerMaster
[2024-03-20T06:28:23.126+0000] {subprocess.py:93} INFO - 24/03/20 06:28:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-03-20T06:28:23.127+0000] {subprocess.py:93} INFO - 24/03/20 06:28:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-03-20T06:28:25.255+0000] {subprocess.py:93} INFO - 24/03/20 06:28:21 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:8032
[2024-03-20T06:28:25.257+0000] {subprocess.py:93} INFO - 24/03/20 06:28:22 INFO AHSProxy: Connecting to Application History server at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:10200
[2024-03-20T06:28:27.233+0000] {subprocess.py:93} INFO - 24/03/20 06:28:24 INFO Configuration: resource-types.xml not found
[2024-03-20T06:28:27.234+0000] {subprocess.py:93} INFO - 24/03/20 06:28:24 INFO ResourceUtils: Unable to find 'resource-types.xml'.
[2024-03-20T06:28:29.564+0000] {subprocess.py:93} INFO - 24/03/20 06:28:26 INFO YarnClientImpl: Submitted application application_1710915649067_0001
[2024-03-20T06:28:32.323+0000] {subprocess.py:93} INFO - 24/03/20 06:28:28 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at dez-cluster-m.us-central1-b.c.dez-workspace-emil.internal./10.128.0.6:8030
[2024-03-20T06:28:34.001+0000] {subprocess.py:93} INFO - 24/03/20 06:28:31 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=457; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/830261e5-2d3d-4491-a80c-fd201b1ad541/spark-job-history
[2024-03-20T06:28:34.003+0000] {subprocess.py:93} INFO - 24/03/20 06:28:31 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2024-03-20T06:28:34.004+0000] {subprocess.py:93} INFO - 24/03/20 06:28:31 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=176; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-329749248489-tvazyaju/830261e5-2d3d-4491-a80c-fd201b1ad541/spark-job-history
[2024-03-20T06:28:36.083+0000] {subprocess.py:93} INFO - READING CSV FILE
[2024-03-20T06:28:43.195+0000] {subprocess.py:93} INFO - TRANSFORMING DATA
[2024-03-20T06:28:45.230+0000] {subprocess.py:93} INFO - WRITING DATA
[2024-03-20T06:28:45.232+0000] {subprocess.py:93} INFO - 24/03/20 06:28:44 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=243; previousMaxLatencyMs=0; operationCount=1; context=gs://landing_bucket_dez/idm.csv
[2024-03-20T06:28:55.773+0000] {subprocess.py:93} INFO - 24/03/20 06:28:54 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (dez-cluster-w-0.us-central1-b.c.dez-workspace-emil.internal executor 1): java.io.FileNotFoundException:
[2024-03-20T06:28:55.774+0000] {subprocess.py:93} INFO - Item not found: 'gs://landing_bucket_dez/idm.csv'. Note, it is possible that the live version is still available but the requested generation is deleted.
[2024-03-20T06:28:55.774+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:55.774+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate
[2024-03-20T06:28:55.775+0000] {subprocess.py:93} INFO - the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
[2024-03-20T06:28:55.775+0000] {subprocess.py:93} INFO - recreating the Dataset/DataFrame involved.
[2024-03-20T06:28:55.775+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:55.776+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)
[2024-03-20T06:28:55.776+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
[2024-03-20T06:28:55.776+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
[2024-03-20T06:28:55.776+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
[2024-03-20T06:28:55.777+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:55.777+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2024-03-20T06:28:55.777+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2024-03-20T06:28:55.777+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2024-03-20T06:28:55.778+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:55.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
[2024-03-20T06:28:55.778+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:55.778+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:55.779+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)
[2024-03-20T06:28:55.779+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)
[2024-03-20T06:28:55.779+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:55.779+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2024-03-20T06:28:55.780+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2024-03-20T06:28:55.780+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2024-03-20T06:28:55.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
[2024-03-20T06:28:55.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
[2024-03-20T06:28:55.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)
[2024-03-20T06:28:55.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
[2024-03-20T06:28:55.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)
[2024-03-20T06:28:55.781+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.604+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 ERROR TaskSetManager: Task 2 in stage 0.0 failed 4 times; aborting job
[2024-03-20T06:28:57.604+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 WARN TaskSetManager: Lost task 1.3 in stage 0.0 (TID 15) (dez-cluster-w-1.us-central1-b.c.dez-workspace-emil.internal executor 2): TaskKilled (Stage cancelled)
[2024-03-20T06:28:57.604+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 WARN TaskSetManager: Lost task 3.3 in stage 0.0 (TID 14) (dez-cluster-w-0.us-central1-b.c.dez-workspace-emil.internal executor 1): TaskKilled (Stage cancelled)
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 ERROR FileFormatWriter: Aborting job 697453cc-02a9-4a4a-8448-34e25b35d0a4.
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 12) (dez-cluster-w-1.us-central1-b.c.dez-workspace-emil.internal executor 2): java.io.FileNotFoundException:
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - Item not found: 'gs://landing_bucket_dez/idm.csv'. Note, it is possible that the live version is still available but the requested generation is deleted.
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate
[2024-03-20T06:28:57.605+0000] {subprocess.py:93} INFO - the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - recreating the Dataset/DataFrame involved.
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
[2024-03-20T06:28:57.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2024-03-20T06:28:57.608+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
[2024-03-20T06:28:57.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.610+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:257) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
[2024-03-20T06:28:57.617+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.618+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:829) ~[?:?]
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException:
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - Item not found: 'gs://landing_bucket_dez/idm.csv'. Note, it is possible that the live version is still available but the requested generation is deleted.
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.619+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - recreating the Dataset/DataFrame involved.
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660) ~[spark-catalyst_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.621+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.622+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.18.jar:?]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53) ~[spark-sql_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265) ~[spark-core_2.12-3.3.2.jar:3.3.2]
[2024-03-20T06:28:57.623+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://landing_bucket_dez/idm.csv/' directory.
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO - 24/03/20 06:28:55 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=387; previousMaxLatencyMs=243; operationCount=2; context=gs://landing_bucket_dez/idm.csv/_temporary
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO -   File "/tmp/7c37081032174b04ac27a2123f3430be/csv_to_pq.py", line 112, in <module>
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO -     .parquet(LANDING_BUCKET + CSV_FILE)
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1140, in parquet
[2024-03-20T06:28:57.624+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o110.parquet.
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted.
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:650)
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:309)
[2024-03-20T06:28:57.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2024-03-20T06:28:57.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2024-03-20T06:28:57.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2024-03-20T06:28:57.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2024-03-20T06:28:57.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-03-20T06:28:57.630+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2024-03-20T06:28:57.631+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 12) (dez-cluster-w-1.us-central1-b.c.dez-workspace-emil.internal executor 2): java.io.FileNotFoundException:
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - Item not found: 'gs://landing_bucket_dez/idm.csv'. Note, it is possible that the live version is still available but the requested generation is deleted.
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
[2024-03-20T06:28:57.632+0000] {subprocess.py:93} INFO - recreating the Dataset/DataFrame involved.
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
[2024-03-20T06:28:57.633+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.634+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2024-03-20T06:28:57.635+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.636+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2024-03-20T06:28:57.637+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)
[2024-03-20T06:28:57.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:257)
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - 	... 42 more
[2024-03-20T06:28:57.639+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException:
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - Item not found: 'gs://landing_bucket_dez/idm.csv'. Note, it is possible that the live version is still available but the requested generation is deleted.
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - It is possible the underlying files have been updated. You can explicitly invalidate
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - recreating the Dataset/DataFrame involved.
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:28:57.640+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:660)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2024-03-20T06:28:57.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)
[2024-03-20T06:28:57.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)
[2024-03-20T06:28:57.643+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)
[2024-03-20T06:28:57.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)
[2024-03-20T06:28:57.644+0000] {subprocess.py:93} INFO - 	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)
[2024-03-20T06:28:57.644+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:29:01.188+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [7c37081032174b04ac27a2123f3430be] failed with error:
[2024-03-20T06:29:01.191+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2024-03-20T06:29:01.192+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/7c37081032174b04ac27a2123f3430be?project=dez-workspace-emil&region=us-central1
[2024-03-20T06:29:01.192+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait '7c37081032174b04ac27a2123f3430be' --region 'us-central1' --project 'dez-workspace-emil'
[2024-03-20T06:29:01.193+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/830261e5-2d3d-4491-a80c-fd201b1ad541/jobs/7c37081032174b04ac27a2123f3430be/
[2024-03-20T06:29:01.194+0000] {subprocess.py:93} INFO - gs://dataproc-staging-us-central1-329749248489-jq1oe9c7/google-cloud-dataproc-metainfo/830261e5-2d3d-4491-a80c-fd201b1ad541/jobs/7c37081032174b04ac27a2123f3430be/driveroutput.*
[2024-03-20T06:29:01.284+0000] {subprocess.py:93} INFO - 
[2024-03-20T06:29:01.284+0000] {subprocess.py:93} INFO - DONE!
[2024-03-20T06:29:01.285+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2024-03-20T06:29:01.298+0000] {taskinstance.py:1149} INFO - Marking task as SUCCESS. dag_id=IDM_TO_BQ_PQ_RAW, task_id=csv_to_pq, execution_date=20240310T000000, start_date=20240320T062804, end_date=20240320T062901
[2024-03-20T06:29:01.348+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-03-20T06:29:01.359+0000] {taskinstance.py:3312} INFO - 1 downstream tasks scheduled from follow-on schedule check
